Oisin Doherty
oisind
#1269085

Individual Status Report – Week 8

“Primarily, our goal is to get a working version of our system set up for Thursday. This requires me to have our scraper parsing the oldest (and most likely well-answered) StackOverflow questions autonomously and have a good output format for our verification files. After we get it working and set up completely, I can work on verifying that the bash commands that we have gathered are valid. As a longer term goal if we have extra time towards the end, we can attempt to integrate another source for natural language / command pairs into our user interface.”

Strictly speaking, we did get initial results tested for Thursday. One of the components of getting a working pipeline was to use a data format that didn’t rely on multiple delimiters; ultimately, we ended up settling on JSON because of how easy it is to encode and decode. While we didn’t end up getting a true ‘pipeline’ set up for last week --we didn’t have time to update the website to reflect these changes in format-- we have time to fix it this week. I also changed the scraper to actually scrape through all the questions in StackOverflow related to bash/shell rather than just the 50 most popular ones intended for gathering training data. There are currently 25,000 questions tagged either ‘shell’ or ‘bash’ which we can scrape at a rate of around one per second. This means that we can get an initial database of questions in around seven hours. I would like to do a sweeping overview of the final report as we get closer to our final revisions. Most of the content we have in there is good, but responding to feedback from other groups and I think that further organizing our content would do well to make our report more readable. Most of the groundwork has been set and, not to jinx anything, we’re more or less going through the motions in finalizing our system. 

We’re closing in on the end of this course, meaning that we really need to get a working pipeline complete soon. We’re still making design decisions that we should have figured out earlier in the planning process, but we’re still making forward progress. The most important thing to get done is getting the JSON output of the scraper to work with the website. For a longer-term goal, I should consider some heuristic we can use to make sure that we don’t scrape pages that are unlikely to get new information on subsequent sweeps (such as the most popular, thoroughly vetted questions).
